{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "# Lip-to-Speech Synthesis with LLMs and Audio-Visual Instructions\n",
        "\n",
        "This notebook demonstrates an end-to-end lip-to-speech synthesis pipeline that generates speech from silent lip videos, using large language models (LLMs) and audio-visual instructions. The project combines techniques from the papers [\"AVI-Talking\"](https://arxiv.org/abs/2402.16124) and [\"Towards Accurate Lip-to-Speech Synthesis in-the-Wild\"](https://arxiv.org/abs/2403.01087) to achieve this task.\n",
        "\n",
        "The primary goal of this project is to enable natural speech synthesis from lip movements, without requiring any audio input or transcripts. This technology has potential applications in various domains, including virtual assistants, video dubbing, and accessibility tools for individuals with speech impairments.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The lip-to-speech synthesis process is divided into two main stages:\n",
        "\n",
        "1. **Audio-Visual Instruction Generation**: In this stage, a pre-trained lipreading model extracts visual features from the input lip video. These visual features are then combined with noisy text transcripts (generated by the lipreading model) and fed into a fine-tuned large language model (LLM). The LLM generates audio-visual instructions that describe the lip movements and the corresponding speech content.\n",
        "2. **Visual Text-to-Speech Synthesis**: In this stage, the audio-visual instructions are used to condition a visual text-to-speech model, which generates a mel-spectrogram representation of the speech. This mel-spectrogram is then converted into an audio waveform using a vocoder (e.g., ParallelWaveGAN), while considering the target speaker's voice characteristics.\n",
        "\n",
        "## Usage\n",
        "\n",
        "To use this project, you will need to provide two input files:\n",
        "\n",
        "1. `input_lip_video.mp4`: This is the silent video for lip reading. A video file containing clear lip movements of the person speaking. The video should be reasonably well-lit, with the speaker's lips and mouth area clearly visible. Ideally, the video should have a resolution of at least 720p and a framerate of at least 30 FPS.\n",
        "2. `target_speaker_audio.wav`: This is the reference audio sample for recreating the speaker voice. A short audio clip (e.g., 5-10 seconds) of the target speaker's voice. This audio clip will be used to extract the speaker's voice characteristics, which will be incorporated into the synthesized speech.\n",
        "\n",
        "With these input files, you can run the notebook cells in sequential order to generate synthesized speech from the lip video, while preserving the target speaker's voice characteristics.\n",
        "\n",
        "Please note that this project requires access to pre-trained models and libraries, which are automatically downloaded and set up within the notebook. Additionally, some components of the pipeline may require significant computational resources, particularly for processing longer videos or achieving real-time performance.\n",
        "\n",
        "By leveraging the power of LLMs and audio-visual instructions, this project aims to push the boundaries of lip-to-speech synthesis, enabling more natural and expressive speech generation from visual cues alone."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and initialization..."
      ],
      "metadata": {
        "id": "bY6nNxW2PuDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "hf_access_token = \"REPLACE WITH YOUR HUGGING FACE TOKEN\""
      ],
      "metadata": {
        "id": "RBZ1FwtwuXbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "WMHPkEMCVsSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "!pip install parallel_wavegan\n",
        "from parallel_wavegan.models import ParallelWaveGANGenerator\n",
        "\n",
        "!pip install phonemizer\n",
        "from phonemizer import phonemize\n",
        "\n",
        "!pip install speechbrain\n",
        "from speechbrain.inference import EncoderClassifier\n",
        "!mkdir pretrained_models\n",
        "\n",
        "!pip install opencv-python\n",
        "import cv2\n",
        "\n",
        "!pip install numpy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that GPU is available with Tensorflow GPU -\n",
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs:\", len(physical_devices))"
      ],
      "metadata": {
        "id": "i2NkCH21JOab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --recursive https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks.git\n",
        "%cd Lipreading_using_Temporal_Convolutional_Networks\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "JN94ZKmjCrDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Lipreading_using_Temporal_Convolutional_Networks\n",
        "import gdown\n",
        "gdown.download('https://drive.usercontent.google.com/download?id=1TGFG0dW5M3rBErgU8i0N7M1ys9YMIvgm&export=download&authuser=0&confirm=t&uuid=69f4c2bc-42ca-461c-a61f-56b4e7dc601b&at=APZUnTWa0cWQMJsRReOodH4XyVIP%3A1710192153986', output='models/model_weights.pth', fuzzy=True)"
      ],
      "metadata": {
        "id": "ZM1vsUhiV1ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required modules\n",
        "!pip install torch torchvision\n",
        "\n",
        "# Import the required modules\n",
        "import torch\n",
        "from lipreading.utils import load_model, calculateNorm2, load_json\n",
        "from lipreading.model import Lipreading"
      ],
      "metadata": {
        "id": "DlCLE07snQ59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "import urllib.request, json\n",
        "\n",
        "def get_lrw_model_from_json(json_config_url):\n",
        "  response = urllib.request.urlopen(json_config_url)\n",
        "  args_loaded = json.loads(response.read())\n",
        "  backbone_type = args_loaded['backbone_type']\n",
        "  width_mult = args_loaded['width_mult']\n",
        "  relu_type = args_loaded['relu_type']\n",
        "  use_boundary = args_loaded.get(\"use_boundary\", False)\n",
        "\n",
        "  if args_loaded.get('tcn_num_layers', ''):\n",
        "    tcn_options = { 'num_layers': args_loaded['tcn_num_layers'],\n",
        "                    'kernel_size': args_loaded['tcn_kernel_size'],\n",
        "                    'dropout': args_loaded['tcn_dropout'],\n",
        "                    'dwpw': args_loaded['tcn_dwpw'],\n",
        "                    'width_mult': args_loaded['tcn_width_mult'],\n",
        "                  }\n",
        "  else:\n",
        "    tcn_options = {}\n",
        "  if args_loaded.get('densetcn_block_config', ''):\n",
        "    densetcn_options = {'block_config': args_loaded['densetcn_block_config'],\n",
        "                        'growth_rate_set': args_loaded['densetcn_growth_rate_set'],\n",
        "                        'reduced_size': args_loaded['densetcn_reduced_size'],\n",
        "                        'kernel_size_set': args_loaded['densetcn_kernel_size_set'],\n",
        "                        'dilation_size_set': args_loaded['densetcn_dilation_size_set'],\n",
        "                        'squeeze_excitation': args_loaded['densetcn_se'],\n",
        "                        'dropout': args_loaded['densetcn_dropout'],\n",
        "                        }\n",
        "  else:\n",
        "    densetcn_options = {}\n",
        "\n",
        "  model = Lipreading( modality='video',\n",
        "                      num_classes=500,\n",
        "                      tcn_options=tcn_options,\n",
        "                      densetcn_options=densetcn_options,\n",
        "                      backbone_type=backbone_type,\n",
        "                      relu_type=relu_type,\n",
        "                      width_mult=width_mult,\n",
        "                      use_boundary=use_boundary,\n",
        "                      extract_feats=False).cuda()\n",
        "  calculateNorm2(model)\n",
        "  return model\n",
        "\n",
        "# Get the model with default parameters\n",
        "json_config_url = 'https://raw.githubusercontent.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks/master/configs/lrw_resnet18_dctcn_boundary.json'\n",
        "lrw_model = get_lrw_model_from_json(json_config_url)\n",
        "\n",
        "# Load the model weights\n",
        "lrw_model = load_model(\"/content/Lipreading_using_Temporal_Convolutional_Networks/models/model_weights.pth\", lrw_model)\n",
        "lrw_model.eval()"
      ],
      "metadata": {
        "id": "GeCq1oHkUkrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_datasets"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Download and load the LLama2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=hf_access_token)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=hf_access_token)\n",
        "\n",
        "# Download the BigVGAN vocoder\n",
        "bigvgan_url = 'https://github.com/PlayVoice/BigVGAN/releases/download/augment/nsf_bigvgan_pretrain_32K_Augment.pth'\n",
        "bigvgan_output = 'bigvgan_pretrained.pth'\n",
        "gdown.download(bigvgan_url, bigvgan_output, quiet=False)\n",
        "\n",
        "# Download the CMU phoneme vocabulary\n",
        "cmu_url = 'http://www.speech.cs.cmu.edu/cgi-bin/cmudict/cmudict.0.7a'\n",
        "cmu_output = 'cmudict-0.7b.symbols'\n",
        "gdown.download(cmu_url, cmu_output, quiet=False)\n",
        "\n",
        "print(\"Datasets downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example_usage"
      },
      "source": [
        "## Example Usage\n",
        "\n",
        "Load the pre-trained models and provide an example usage of the lip-to-speech synthesis program.\n",
        "\n",
        "1. [Load the pretrained models](https://colab.research.google.com/drive/1T5ihTJMCaqmOSAcC1xXyKU0CzGLttqHA#scrollTo=download_datasets&line=2&uniqifier=1)\n",
        "2. Load the Video and Audio files.\n",
        "3. Execute the synthesis function.\n",
        "4. Save the new audio file.\n",
        "5. [Optional] Use ffmpeg to merge the audio and file files (not included in this project)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example_usage_code"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "# Download the silent lip video\n",
        "silent_video_url = 'https://github.com/zoharbabin/lipsynth-experiment/raw/main/silent_video.mp4'\n",
        "silent_video_filename = 'silent_video.mp4'\n",
        "urllib.request.urlretrieve(silent_video_url, silent_video_filename)\n",
        "print(f'Silent lip video downloaded as {silent_video_filename}')\n",
        "# Download the target speaker audio\n",
        "target_audio_url = 'https://github.com/zoharbabin/lipsynth-experiment/raw/main/reference_audio.wav'\n",
        "target_audio_filename = 'reference_audio.wav'\n",
        "urllib.request.urlretrieve(target_audio_url, target_audio_filename)\n",
        "print(f'Target speaker audio downloaded as {target_audio_filename}')\n",
        "\n",
        "# Example usage\n",
        "lip_video = load_video('silent_video.mp4')\n",
        "target_speaker_audio = 'reference_audio.wav'\n",
        "\n",
        "generated_speech = lip_to_speech_synthesis(lip_video, target_speaker_audio)\n",
        "\n",
        "# Save the generated speech\n",
        "save_audio(generated_speech, 'generated_speech.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions Below..."
      ],
      "metadata": {
        "id": "MxFfpWM4Pnl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "load_video"
      },
      "outputs": [],
      "source": [
        "def load_video(video_path):\n",
        "    # Load the video frames using OpenCV\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "save_audio"
      },
      "outputs": [],
      "source": [
        "def save_audio(audio, output_path):\n",
        "    # Save the audio using scipy\n",
        "    scipy.io.wavfile.write(output_path, 22050, audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "phoneme_tokenizer"
      },
      "outputs": [],
      "source": [
        "class PhonemeTokenizer:\n",
        "    def __init__(self, vocab_file):\n",
        "        self.vocab = self.load_vocab(vocab_file)\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def load_vocab(self, vocab_file):\n",
        "        with open(vocab_file, 'r') as f:\n",
        "            vocab = f.read().splitlines()\n",
        "        return {p: i for i, p in enumerate(vocab)}\n",
        "\n",
        "    def tokenize(self, phoneme_sequence):\n",
        "        return [self.vocab[p] for p in phoneme_sequence.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "positional_encoding"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "generate_av_instructions"
      },
      "outputs": [],
      "source": [
        "def generate_AV_instructions(lip_video):\n",
        "    # Extract visual features using the pre-trained Lipreading model\n",
        "    visual_features = lrw_model.extract_features(lip_video)\n",
        "\n",
        "    # Generate noisy text transcripts using the Lipreading model\n",
        "    noisy_text_transcripts = lrw_model.predict(lip_video)\n",
        "\n",
        "    # Perform Q-Former alignment\n",
        "    aligned_visual_features = Q_Former_align(visual_features)\n",
        "\n",
        "    # Fine-tune the LLM projection layer\n",
        "    fine_tuned_llm = fine_tune_LLM_projection(llm_model, aligned_visual_features, noisy_text_transcripts)\n",
        "\n",
        "    # Generate audio-visual instructions using the fine-tuned LLM\n",
        "    AV_instructions = fine_tuned_llm.generate(inputs_embeds=aligned_visual_features, text_inputs=noisy_text_transcripts)\n",
        "\n",
        "    return AV_instructions, visual_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qformer_align"
      },
      "outputs": [],
      "source": [
        "class QFormer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers):\n",
        "        super(QFormer, self).__init__()\n",
        "        self.self_attn_layers = nn.ModuleList([nn.TransformerEncoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
        "        self.cross_attn_layers = nn.ModuleList([nn.TransformerDecoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, visual_features, text_features):\n",
        "        # Self-attention on visual features\n",
        "        for self_attn_layer in self.self_attn_layers:\n",
        "            visual_features = self_attn_layer(visual_features)\n",
        "\n",
        "        # Cross-attention between visual features and text features\n",
        "        for cross_attn_layer in self.cross_attn_layers:\n",
        "            visual_features = cross_attn_layer(visual_features, text_features)\n",
        "\n",
        "        # Layer normalization\n",
        "        visual_features = self.norm(visual_features)\n",
        "\n",
        "        return visual_features\n",
        "\n",
        "def Q_Former_align(visual_features, d_model=512, nhead=8, num_layers=4):\n",
        "    # Create an instance of the Q-Former network\n",
        "    q_former = QFormer(d_model, nhead, num_layers)\n",
        "\n",
        "    # Align visual features using Q-Former\n",
        "    aligned_visual_features = q_former(visual_features, None)\n",
        "\n",
        "    return aligned_visual_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "finetune_llm"
      },
      "outputs": [],
      "source": [
        "def fine_tune_LLM_projection(llm_model, aligned_visual_features, noisy_text_transcript, num_epochs=5, batch_size=4, learning_rate=1e-5):\n",
        "    # Tokenize the noisy text transcript\n",
        "    input_ids = llm_tokenizer(noisy_text_transcript, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
        "\n",
        "    # Create a DataLoader for batching the data\n",
        "    dataset = torch.utils.data.TensorDataset(aligned_visual_features, input_ids)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Set the LLM to training mode\n",
        "    llm_model.train()\n",
        "\n",
        "    # Freeze all layers except the input projection layer\n",
        "    for name, param in llm_model.named_parameters():\n",
        "        if 'embed_in' not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Create an optimizer for the input projection layer\n",
        "    optimizer = torch.optim.AdamW(llm_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Fine-tune the input projection layer\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in dataloader:\n",
        "            batch_visual_features, batch_input_ids = batch\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = llm_model(inputs_embeds=batch_visual_features, labels=batch_input_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return llm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "text_to_phonemes"
      },
      "outputs": [],
      "source": [
        "def text_to_phonemes(text):\n",
        "    phonemes = phonemize(text, language='en-us', backend='espeak', strip=True)\n",
        "    phoneme_sequence = ' '.join(phonemes)\n",
        "    return phoneme_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "encode_text"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, dropout=0.1):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src) * math.sqrt(d_model)\n",
        "        src = self.positional_encoding(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        return output\n",
        "\n",
        "def encode_text(phoneme_sequence, vocab_size, d_model=512, nhead=8, num_layers=6, dropout=0.1):\n",
        "    # Tokenize the phoneme sequence\n",
        "    tokenizer = PhonemeTokenizer('cmudict-0.7b.symbols')\n",
        "    tokens = tokenizer.tokenize(phoneme_sequence)\n",
        "\n",
        "    # Convert tokens to tensor\n",
        "    input_tensor = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Create an instance of the TextEncoder\n",
        "    text_encoder = TextEncoder(vocab_size, d_model, nhead, num_layers, dropout)\n",
        "\n",
        "    # Encode the phoneme sequence\n",
        "    phoneme_embeddings = text_encoder(input_tensor)\n",
        "\n",
        "    return phoneme_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "encode_visual"
      },
      "outputs": [],
      "source": [
        "class VisualEncoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, dropout=0.1):\n",
        "        super(VisualEncoder, self).__init__()\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.positional_encoding(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        return output\n",
        "\n",
        "def encode_visual(visual_features, d_model=512, nhead=8, num_layers=6, dropout=0.1):\n",
        "    # Ensure the visual features have the correct dimensions\n",
        "    assert visual_features.ndim == 3, \"Visual features should have dimensions (batch_size, sequence_length, feature_dim)\"\n",
        "\n",
        "    # Create an instance of the VisualEncoder\n",
        "    visual_encoder = VisualEncoder(d_model, nhead, num_layers, dropout)\n",
        "\n",
        "    # Encode the visual features\n",
        "    visual_embeddings = visual_encoder(visual_features)\n",
        "\n",
        "    return visual_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "align_embeddings"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"Compute scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        query (torch.Tensor): Query tensor of shape (batch_size, num_heads, query_length, d_k).\n",
        "        key (torch.Tensor): Key tensor of shape (batch_size, num_heads, key_length, d_k).\n",
        "        value (torch.Tensor): Value tensor of shape (batch_size, num_heads, value_length, d_v).\n",
        "        mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, key_length). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor of shape (batch_size, num_heads, query_length, d_v).\n",
        "        torch.Tensor: Attention weights tensor of shape (batch_size, num_heads, query_length, key_length).\n",
        "    \"\"\"\n",
        "    d_k = key.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    attention_weights = nn.functional.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    return output, attention_weights\n",
        "\n",
        "def align_embeddings(phoneme_embeddings, visual_embeddings, d_model=512, nhead=8, dropout=0.1):\n",
        "    \"\"\"Align phoneme embeddings with visual embeddings using scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        phoneme_embeddings (torch.Tensor): Phoneme embeddings tensor of shape (batch_size, phoneme_seq_length, d_model).\n",
        "        visual_embeddings (torch.Tensor): Visual embeddings tensor of shape (batch_size, visual_seq_length, d_model).\n",
        "        d_model (int, optional): Model dimension. Defaults to 512.\n",
        "        nhead (int, optional): Number of attention heads. Defaults to 8.\n",
        "        dropout (float, optional): Dropout probability. Defaults to 0.1.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Aligned embeddings tensor of shape (batch_size, phoneme_seq_length, d_model).\n",
        "    \"\"\"\n",
        "    # Ensure the embeddings have the correct dimensions\n",
        "    assert phoneme_embeddings.ndim == 3 and visual_embeddings.ndim == 3, \"Embeddings should have dimensions (batch_size, sequence_length, embedding_dim)\"\n",
        "    assert phoneme_embeddings.size(-1) == d_model and visual_embeddings.size(-1) == d_model, \"Embeddings should have the same feature dimension as d_model\"\n",
        "\n",
        "    # Create query, key, and value tensors\n",
        "    query = phoneme_embeddings\n",
        "    key = visual_embeddings\n",
        "    value = visual_embeddings\n",
        "\n",
        "    # Split the embeddings into heads\n",
        "    batch_size, seq_len, _ = query.size()\n",
        "    query = query.view(batch_size, seq_len, nhead, d_model // nhead).transpose(1, 2)\n",
        "    key = key.view(batch_size, seq_len, nhead, d_model // nhead).transpose(1, 2)\n",
        "    value = value.view(batch_size, seq_len, nhead, d_model // nhead).transpose(1, 2)\n",
        "\n",
        "    # Compute the scaled dot-product attention\n",
        "    aligned_embeddings, _ = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "    # Concatenate the heads and apply dropout\n",
        "    aligned_embeddings = aligned_embeddings.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "    aligned_embeddings = nn.functional.dropout(aligned_embeddings, p=dropout)\n",
        "\n",
        "    return aligned_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "upsample"
      },
      "outputs": [],
      "source": [
        "class UpsampleNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, upsample_factors):\n",
        "        super(UpsampleNetwork, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.upsample_factors = upsample_factors\n",
        "\n",
        "        self.upsample_layers = nn.ModuleList()\n",
        "        for factor in upsample_factors:\n",
        "            self.upsample_layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose1d(input_dim, output_dim, kernel_size=factor, stride=factor),\n",
        "                    nn.BatchNorm1d(output_dim),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            )\n",
        "            input_dim = output_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.upsample_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "def upsample(aligned_embeddings, output_dim, upsample_factors):\n",
        "    # Ensure the aligned embeddings have the correct dimensions\n",
        "    assert aligned_embeddings.ndim == 3, \"Aligned embeddings should have dimensions (batch_size, sequence_length, embedding_dim)\"\n",
        "\n",
        "    # Create an instance of the UpsampleNetwork\n",
        "    input_dim = aligned_embeddings.size(-1)\n",
        "    upsample_network = UpsampleNetwork(input_dim, output_dim, upsample_factors)\n",
        "\n",
        "    # Upsample the aligned embeddings\n",
        "    upsampled_embeddings = upsample_network(aligned_embeddings.transpose(1, 2))\n",
        "    upsampled_embeddings = upsampled_embeddings.transpose(1, 2)\n",
        "\n",
        "    return upsampled_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "decode_melspectrogram"
      },
      "outputs": [],
      "source": [
        "class MelDecoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, output_dim, dropout=0.1):\n",
        "        super(MelDecoder, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dropout=dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        output = self.transformer_decoder(tgt, memory)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "def decode_melspectrogram(conditioned_embeddings, d_model, nhead, num_layers, output_dim, dropout=0.1):\n",
        "    # Ensure the conditioned embeddings have the correct dimensions\n",
        "    assert conditioned_embeddings.ndim == 3, \"Conditioned embeddings should have dimensions (batch_size, sequence_length, embedding_dim)\"\n",
        "\n",
        "    # Create target tensor for decoder input\n",
        "    batch_size, seq_len, _ = conditioned_embeddings.size()\n",
        "    tgt = torch.zeros(batch_size, seq_len, d_model, device=conditioned_embeddings.device)\n",
        "\n",
        "    # Create an instance of the MelDecoder\n",
        "    mel_decoder = MelDecoder(d_model, nhead, num_layers, output_dim, dropout)\n",
        "\n",
        "    # Decode the mel-spectrogram\n",
        "    generated_melspectrogram = mel_decoder(tgt, conditioned_embeddings)\n",
        "\n",
        "\n",
        "    return generated_melspectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "extract_speaker_embedding"
      },
      "outputs": [],
      "source": [
        "def extract_speaker_embedding(audio_path):\n",
        "    # Load the pre-trained speaker recognition model\n",
        "    speaker_recognition = EncoderClassifier.from_hparams(\n",
        "        source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "        savedir=\"pretrained_models/spkrec-ecapa-voxceleb\"\n",
        "    )\n",
        "\n",
        "    # Load the audio file\n",
        "    signal, fs = torchaudio.load(audio_path)\n",
        "\n",
        "    # Extract the speaker embedding\n",
        "    embeddings = speaker_recognition.encode_batch(signal)\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "synthesize_speech"
      },
      "outputs": [],
      "source": [
        "def synthesize_speech(AV_instructions, visual_features, target_speaker_embedding):\n",
        "    # Tokenize AV instructions and create phoneme sequence\n",
        "    phoneme_sequence = text_to_phonemes(AV_instructions)\n",
        "\n",
        "    if not phoneme_sequence:\n",
        "        print(\"Warning: Empty or invalid phoneme sequence. Skipping speech synthesis.\")\n",
        "        return None\n",
        "\n",
        "    # Encode phoneme sequence using a Transformer encoder\n",
        "    phoneme_tokenizer = PhonemeTokenizer('/content/cmudict-0.7b.symbols')\n",
        "    phoneme_tokens = phoneme_tokenizer.tokenize(phoneme_sequence)\n",
        "    phoneme_embeddings = encode_text(phoneme_tokens, phoneme_tokenizer.vocab_size)\n",
        "\n",
        "    # Encode visual features using a Transformer encoder\n",
        "    visual_embeddings = encode_visual(visual_features)\n",
        "\n",
        "    # Align phoneme and visual embeddings using scaled dot-product attention\n",
        "    aligned_embeddings = align_embeddings(phoneme_embeddings, visual_embeddings)\n",
        "\n",
        "    # Upsample aligned embeddings to match the desired mel-spectrogram length\n",
        "    upsampled_embeddings = upsample(aligned_embeddings, output_dim=80, upsample_factors=[2, 2, 2, 2, 5])\n",
        "\n",
        "    # Concatenate upsampled embeddings with target speaker embedding\n",
        "    conditioned_embeddings = torch.cat((upsampled_embeddings, target_speaker_embedding.unsqueeze(0).repeat(upsampled_embeddings.size(0), 1, 1)), dim=-1)\n",
        "\n",
        "    # Decode conditioned embeddings into a mel-spectrogram using a Transformer decoder\n",
        "    generated_melspectrogram = decode_melspectrogram(conditioned_embeddings, d_model=512, nhead=8, num_layers=6, output_dim=80)\n",
        "\n",
        "    # Convert mel-spectrogram to speech waveform using the ParallelWaveGANGenerator\n",
        "    vocoder = ParallelWaveGANGenerator(model_dir='/content/')\n",
        "    with torch.no_grad():\n",
        "        generated_speech = vocoder.inference(generated_melspectrogram)\n",
        "\n",
        "    return generated_speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lip_to_speech_synthesis"
      },
      "outputs": [],
      "source": [
        "def lip_to_speech_synthesis(lip_video, target_speaker_audio):\n",
        "    # Generate audio-visual instructions using LLMs (Stage 1)\n",
        "    AV_instructions, visual_features = generate_AV_instructions(lip_video)\n",
        "\n",
        "    # Extract target speaker embedding from a short audio clip\n",
        "    target_speaker_embedding = extract_speaker_embedding(target_speaker_audio)\n",
        "\n",
        "    # Synthesize speech using visual text-to-speech model (Stage 2)\n",
        "    generated_speech = synthesize_speech(AV_instructions, visual_features, target_speaker_embedding)\n",
        "\n",
        "    if generated_speech is None:\n",
        "        print(\"Warning: Speech synthesis failed. Returning None.\")\n",
        "        return None\n",
        "\n",
        "    return generated_speech"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}